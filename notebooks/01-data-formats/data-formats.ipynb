{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdfecb05-bb4d-4868-91b8-856d16dd23f5",
   "metadata": {},
   "source": [
    "<img src=\"images/banner.png\" style=\"width: 100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85109dd6",
   "metadata": {},
   "source": [
    "# Working With Different Data Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd21a27a",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[1] Visochek, Allan. *Practical Data Wrangling: Expert techniques for transforming your raw data into a valuable source for analytics.* Packt Publishing Ltd, 2017.\n",
    "\n",
    "[2] McKinney, Wes. *Python for data analysis.* \" O'Reilly Media, Inc.\", 2022.\n",
    "\n",
    "[3] pandas documentation - https://pandas.pydata.org/docs/\n",
    "\n",
    "[4] Revised and grammar checked using ChatGPT - https://chatgpt.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44d7c7-bfdf-4252-b828-050f6365e0b3",
   "metadata": {},
   "source": [
    "Prepared by: Leodegario Lorenzo II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac0516-68f0-46cc-80bf-f4ac29099c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f9edd-7063-4f05-980f-c0b960f2c7eb",
   "metadata": {},
   "source": [
    "Data encountered in real-world analysis varies in how structured it is - ranging from **structured** to **semi-structured** and **unstructured** formats. Preparing these data formats for analysis requires appropriate tools and data wrangling techniques. In this notebook, we explore how to work with structured and semi-structured data using Python, focusing on commonly used formats such as CSV, Excel, and JSON files. We will mainly use `pandas` and Python’s standard library as we we walk through fundamental data wrangling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3d791",
   "metadata": {},
   "source": [
    "## 1 Comma-Separated Values (CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d2c46f-92ee-469f-af8b-19b1f0eacc11",
   "metadata": {},
   "source": [
    "The comma-separated values (CSV) or `csv` format is a text file format used to store tabular data consisting of values separated by commas. Each data entry is represented by a single line and by convention contains column headers as its first row.\n",
    "\n",
    "As an example of a `csv` file, we are given the different player statistics for the NBA Season 2024 to 2025 in `nba_24_25.csv`. A quick inspection of the file yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c07d1b-45b1-4573-bad4-e21bf522af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/nba_24_25.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a13e2aa-2cb7-4d77-aefb-6d1b89bbf1e0",
   "metadata": {},
   "source": [
    "The data is relatively clean with minimal errors in data entry. We can load this `csv` file into our notebook using the `pandas` function `read_csv` as a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b15e82-b037-4982-8f1a-d24e87db1af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_24_25 = pd.read_csv('data/nba_24_25.csv')\n",
    "nba_24_25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c82a389-ff4f-42f7-af29-2b5ab5a1617c",
   "metadata": {},
   "source": [
    "Notice that `pandas` automatically parses the first row in our `csv` as the column header. The index is also automatically created which corresponds to the row number of the data entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33feb8e2-d4af-43b6-a331-03005f085885",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_24_25.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64868c1-6448-480f-a49f-3b7f81241366",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_24_25.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2aa087-c6d5-44cf-9488-b942d6d5ccba",
   "metadata": {},
   "source": [
    "Suppose that we want to use the player's name as index, we can set the index column by specifying the `index_col` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190b97e-3036-4df0-97a3-faf2a405d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_24_25 = pd.read_csv('data/nba_24_25.csv', index_col='Player')\n",
    "nba_24_25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178bc585-89d4-404a-b504-84ba11a6ee90",
   "metadata": {},
   "source": [
    "Some other parameters worth exploring and understanding for the `read_csv` function are - `delimeter`, `usecols`, `header`, `dtype`, `skiprows`, `nrows`, `na_values`, `encoding`, and `parse_dates`. See the documentation of **[pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)** for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9476ca17-08a2-4dfa-bee2-f475980c4e43",
   "metadata": {},
   "source": [
    "Let's demonstrate some essential `pandas` functionalities and a quick data analysis with the `csv` file provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eba9ff-bc1a-46fc-9b96-8364920368a1",
   "metadata": {},
   "source": [
    "Let’s begin by taking a look at the data types `pandas` inferred for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d20ea3b-e495-430c-b0f7-91d20233ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_24_25.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a735b4-9acd-41cd-8e3e-414ad6bb4d24",
   "metadata": {},
   "source": [
    "The data type inside each column has been properly inferred except for the `Data` column, which should have been a date time format. We can modify this by directly working on the data frame `nba_24_25`. But let's use the functionalities of `read_csv` to handle this siutation. We may also want to change the name of the `Data` column to be `Date` for it to be more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d4591-18bc-4ef7-a19b-73623a9ac232",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_24_25 = (pd.read_csv('data/nba_24_25.csv', index_col='Player',\n",
    "                         parse_dates=['Data'])\n",
    "               .rename(columns={'Data': 'Date'}))\n",
    "nba_24_25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc42d0e-c4b8-4307-91b5-6f7570267ccf",
   "metadata": {},
   "source": [
    "The `Data` column has now been properly parsed as a `datetime` format and renamed to `Date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3133ae93-a40e-48e3-a795-59c45e6fa6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_24_25.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24866d61-2e52-4aaf-85af-e0137528c484",
   "metadata": {},
   "source": [
    "To select certain rows or subsets of the data, we can use the `loc` and `iloc` method of the `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd9c9a-56f2-49c3-877f-dbdb4fcbafb5",
   "metadata": {},
   "source": [
    "For example, to select the data rows of `Stephen Curry`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d13702-dce6-443d-a359-f0f4cfb5df3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nba_24_25.loc['Stephen Curry', :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc7ddb2-5828-42ce-b875-e7d1902417c7",
   "metadata": {},
   "source": [
    "If we only want specific columns, say the `3P` and `3PA`, we specify those columns as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe9222-b77e-4647-9bab-282c844fd448",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nba_24_25.loc['Stephen Curry', ['3P', '3PA']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80400994-e24f-4418-b27d-b0ada678a61a",
   "metadata": {},
   "source": [
    "You can also use a boolean array as your indexer. For example, we can select all instances where a player has scored more than 30 points in a game using the following expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda45d2f-9441-48b9-bd7b-c5e0d6ddeafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_24_25.loc[nba_24_25.PTS >= 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08c2c7-16eb-49e0-b9b4-647b70035b98",
   "metadata": {},
   "source": [
    "To sort the result, we can use the `sort_values` method specifying the proper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c46bf8e-1d00-420c-90fd-b26178bbddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_24_25.loc[nba_24_25.PTS >= 30].sort_values(by='PTS', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8481381c-1d4e-47e5-bb07-047caa95bf0c",
   "metadata": {},
   "source": [
    "You can also chain different conditional statement using the *bitwise* operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c066c2c0-d451-4dee-b2ff-62376e0a4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_24_25.loc[(nba_24_25.PTS >= 30) & (nba_24_25.Tm == 'GSW')].sort_values(by='PTS', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfef923d-626f-4d77-aed9-5a91eaf62a75",
   "metadata": {},
   "source": [
    "To create new columns, we can use `loc` then specify the desired new column as its label while performing an assigning operation for its value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9711bb6-dea2-4aa4-9e45-ea051dd0d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_24_25.loc[:, 'Eff'] = nba_24_25.PTS / nba_24_25.MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01dbea9-fb6d-4455-9065-32802a89ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_24_25.sort_values(by='Eff', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ba26e5-04c9-443d-a197-86ecffaaf660",
   "metadata": {},
   "source": [
    "For a quick look of a several statistical measures for numerical data, we can use the `describe` function of `pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411c63d-ce35-4816-90f8-c2847bcfa114",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_24_25.PTS.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea6ed4-be4a-4ba7-b532-5750df70bd8b",
   "metadata": {},
   "source": [
    "Finally, we show how to perform a quick visualization of our data using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5653d71-3c6b-4606-9524-bbd8e1d8012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = nba_24_25.MP.plot(kind='hist', bins=20)\n",
    "\n",
    "ax.set_xlabel('Minutes Played')\n",
    "ax.spines[['top', 'right']].set_visible(False)\n",
    "ax.set_title(\"Histogram Plot of Minutes Played\", weight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e3f49-db46-47a3-a20f-ae42a03525e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_24_25.loc[nba_24_25.Date == '2024-12-25']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0adccff-d920-47b3-a95a-bbc7caff955e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T13:56:27.983525Z",
     "iopub.status.busy": "2026-01-27T13:56:27.981686Z",
     "iopub.status.idle": "2026-01-27T13:56:28.005895Z",
     "shell.execute_reply": "2026-01-27T13:56:27.991933Z",
     "shell.execute_reply.started": "2026-01-27T13:56:27.983397Z"
    }
   },
   "source": [
    "## 2 Excel Binary Format (XLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2f1d3f-2d42-40b4-bd8f-28d2d34fb91a",
   "metadata": {},
   "source": [
    "The main difference between Excel (`.xls` or `xlsx`) files and `csv` files is taht Excel files use a binary format, meaning they cannot be read directly as plain text. In terms of data structure, however, both formats typically store tabular data composed of rows and columns.\n",
    "\n",
    "Unlike `csv` files, however, an Excel file can contain multiple worksheets within a single file. As such, pandas' `read_excel` function offers similar functionalities to `read_csv`, but also provides an additional option to specify which worksheet to load into a `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae57a8-901c-492e-a46e-3455d4a0674f",
   "metadata": {},
   "source": [
    "To demonstrate some functionalities of pandas' `read_excel`, we will look at an Excel file containing the population data of NCR for May 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88fa18-5298-42bd-bf10-33d549285fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncr = pd.read_excel('data/ncr.xlsx')\n",
    "ncr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9422b3d-3bfe-4619-a201-2b0a3337e8d6",
   "metadata": {},
   "source": [
    "Notice that by default, `read_excel` loads the first sheet in the file. If we want to specifically open a certain sheet, we can specify it through the `sheet_name` parameter.\n",
    "\n",
    "For now, let's work with the first sheet found by `read_excel`. This contains the population count for each city in NCR. However, there's some effort of data cleaning to be made.\n",
    "\n",
    "In essence, what we want are the province, city, and municipality names and their corresponding population, thus we will drop some columns. We can accomplish this in several ways, we will show one such approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341b545-b7c4-4951-aa22-b239e7462937",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncr = (pd.read_excel('data/ncr.xlsx', skiprows=4, usecols=[1, 2],\n",
    "                     names=['Province, City, and Municipality', 'Population'])\n",
    "         .dropna().reset_index(drop=True))\n",
    "ncr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f447405-a929-42ac-b51d-d9a28c32f6b7",
   "metadata": {},
   "source": [
    "We can export this data frame into an Excel file using the `DataFrame` method `to_excel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f74ca-f04e-409d-9a57-e08d19cfc875",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncr.to_excel('data/ncr_cleaned.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4602403-88c3-46f0-b77a-990da6199d97",
   "metadata": {},
   "source": [
    "If we want to write an Excel file with several sheets, we can use pandas' `ExcelWriter` using the `with` statement as a context manager.\n",
    "\n",
    "Let's say for example, we want to separate the data into separate sheets for each administrative level (province, city, municipality). We can accomplish this using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735502da-360b-486e-a670-5f148643adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncr.columns = ['Name', 'Population']\n",
    "with pd.ExcelWriter('data/ncr_sheets.xlsx') as writer:\n",
    "    ncr.loc[ncr.iloc[:, 0].str.contains('REGION')].to_excel(writer, sheet_name='region', index=False)\n",
    "    ncr.loc[ncr.iloc[:, 0].str.contains('CITY')].to_excel(writer, sheet_name='city', index=False)\n",
    "    ncr.loc[ncr.iloc[:, 0].str.contains('PATEROS')].to_excel(writer, sheet_name='municipality', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf42f85-1e8a-430c-b3d2-dea188ab25f8",
   "metadata": {},
   "source": [
    "## 3 JavaScript Object Notation (JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde1ca33-00af-4fdd-b892-1b851a421925",
   "metadata": {},
   "source": [
    "The `JSON` format is an example of a **semi-structured**, **hierarchical** data format. Compared to tabular data, `JSON` is more flexible nad free-form, though it often consists of a collection of records that follow a consistent internal structure. One of the main challenges in working with `JSON` files is understanding this structure and identifying which fields are relevant for extraction. While inspecting the `JSON` file directly can help in smaller examples, this approach becomes quickly impractical as file size and complexity increases.\n",
    "\n",
    "In this section, we demonstrate two approaches for working with JSON files. THe first uses Python's built-in `json` library, while the other approach loads the JSON file directly into `pandas`. The `json` library is especially useful when the structure of the data is unclear or deeply nested, whereas `pandas` is often suitable for JSON files that already resemble tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbce3f1c-713f-43c9-8dbf-1fc7fd463269",
   "metadata": {},
   "source": [
    "### `json` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe0bb1-1c44-4c51-bd25-744f620679a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28251ff1-2b33-4b64-8a04-6bf291c084a4",
   "metadata": {},
   "source": [
    "One thing that we can notice is that JSON text files resembles Python dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e328ac77-6aff-4a01-9e16-75ca14b29847",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_str = \"\"\"{ \n",
    "  \"firstName\": \"John\", \n",
    "  \"lastName\": \"Smith\", \n",
    "  \"isAlive\": true, \n",
    "  \"age\": 27, \n",
    "  \"address\": { \n",
    "    \"streetAddress\": \"21 2nd Street\", \n",
    "    \"city\": \"New York\", \n",
    "    \"state\": \"NY\", \n",
    "    \"postalCode\": \"10021-3100\" \n",
    "  }, \n",
    "  \"phoneNumbers\": [ \n",
    "    { \n",
    "      \"type\": \"home\", \n",
    "      \"number\": \"212 555-1234\" \n",
    "    }, \n",
    "    {\n",
    "      \"type\": \"office\", \n",
    "      \"number\": \"646 555-4567\" \n",
    "    } \n",
    "  ], \n",
    "  \"children\": [], \n",
    "  \"spouse\": null \n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65255411-f9a6-49b8-8039-75b13484cdb7",
   "metadata": {},
   "source": [
    "As such, the way we explore `json` files is similar to how we explore nested `dict` in Python. We can do so by first reading a json formatted string using `json.loads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07823854-035a-4f47-9101-9a4b67efcf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_as_dict = json.loads(json_str)\n",
    "json_as_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44022c5a-787e-4b94-ab67-c6d23131dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(json_as_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa84a7d7-d54d-4d8b-804c-56fa17722002",
   "metadata": {},
   "source": [
    "Now, usually `json` files don't come directly as string objects in Python, but are rather read as `.json` text files.\n",
    "\n",
    "To demonstrate how we read `json` files, we will use a dataset from **Seeclickfix**, a platform to report non-emergency issues to local governments. This dataset contains a series of data entries that represents issue reports using the platform. The sample dataset is saved as `scf_data.json` in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d01a4-eaeb-436a-95d5-925f44c1fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/scf_data.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f12592-1f7f-4dca-a207-9aa0d8bb7afb",
   "metadata": {},
   "source": [
    "Let's first understand the structure of the data by looking at the keys of this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462242b8-0c0f-4dbe-878d-d5524e7baa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd15a7cf-0b58-4c70-bb0f-a968e8ca1ffc",
   "metadata": {},
   "source": [
    "It has three keys - `errors`, `issues`, and `metadata`. Among these, the core data lies in the `issues` key, which makes sense since the dataset contains issue reports. Let's look at what it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa7e2ae-892d-4cdc-95d4-be63bb1d6bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['issues']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f42c4b-a02f-48ff-ac78-d25e09da0601",
   "metadata": {},
   "source": [
    "The issues is a list of data entries represented by Python dictionary. Which we can convert to a `DataFrame` for easier manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75aa60-ab80-4adf-ad79-2d8704b16249",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues = pd.DataFrame(data['issues'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea5895-48d1-488c-b38a-b5a2c9a031d6",
   "metadata": {},
   "source": [
    "We can inspect a single element for a closer inspection of the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504696cf-8754-4003-b884-0eafa80beeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d5464f-be4a-4832-962d-a9acb3ada54e",
   "metadata": {},
   "source": [
    "Depending on the certain analysis that we want to do, we may want to select a subset of the data for further analysis. For this case, say we are concerned with how the `description` of the issue is related with the corresponding `rating` of the user and their location. We thus select columns `created_at`, `address`, `description`, `lat`, `lng`, `rating`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c36d70-92e6-47dc-8654-3b54b1e70743",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_cols = ['created_at', 'address', 'description', 'lat', 'lng', 'rating']\n",
    "\n",
    "df_scf = df_issues.loc[:, select_cols]\n",
    "df_scf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58fd7b-065f-4b9c-89fe-60d87c9c3325",
   "metadata": {},
   "source": [
    "Once again, let's check for the data types of the columns of our data frame. This time, we demonstrate the use of the `.info` method of `DataFrames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde5f95-0583-4778-94ca-c9fd74e24cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb8be1-42fc-4c43-b173-737482572940",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T19:01:16.173201Z",
     "iopub.status.busy": "2026-01-27T19:01:16.172840Z",
     "iopub.status.idle": "2026-01-27T19:01:16.182118Z",
     "shell.execute_reply": "2026-01-27T19:01:16.180976Z",
     "shell.execute_reply.started": "2026-01-27T19:01:16.173174Z"
    }
   },
   "source": [
    "Let's convert the `created_at` column into a datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a9160-7724-4c6e-a5f4-c02bac3a4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scf.created_at = pd.to_datetime(df_scf.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219de2c-bbb4-437c-8be2-9aaaa1402644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155c9533-078c-497d-9170-8490d8741439",
   "metadata": {},
   "source": [
    "Now, let's visualize some simple statistics on our data such as:\n",
    "\n",
    "1. A frequency count of ratings.\n",
    "2. A frequency count of issues having `trash` in their description.\n",
    "3. A frequency count showing the time in which the issue was created at the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f6ce9-3dd7-4813-9c5c-b9311445a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scf.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c99e55-402e-40f0-8173-369aba72e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scf.description.str.lower().str.contains('trash').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ae9dc-999d-450c-b1a2-707651957c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scf.created_at.dt.hour.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99934eed-e788-436e-9083-9618485423a1",
   "metadata": {},
   "source": [
    "### pandas `read_json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940254e-51bc-4fc1-928d-83922a31ea36",
   "metadata": {},
   "source": [
    "If the `json` text file already resembles a tabular structure, we can try to directly load it using pandas' function `read_json`.\n",
    "\n",
    "As an example, let's look at a sample twitter data uploaded as the json file `sample_twitter.json.bz2`. We inspect it first using Python's standard libraries. Here, we will need the `bz2` library to decompress the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4370438-b91f-462b-bd2c-b2d7438c6301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee7bb1-948f-4934-bd33-2fc607cb751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open('data/sample_twitter.json.bz2', 'r') as f:\n",
    "    contents = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f1b3b-c63d-488b-9c6f-f08766777c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(contents[:10_000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08d4f56-f541-4bea-9518-79c5160c00e4",
   "metadata": {},
   "source": [
    "Notice here that the data entry for this file is done per line, wherein each line follows the `json` format. In this case, if we want to use the `json` library to load the data, we will perform a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4b121-c00c-4a51-9f9a-e7c5cb597870",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_json = [json.loads(line) for line in contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb92369-708b-46ea-a5d8-4db612272582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T19:50:15.505489Z",
     "iopub.status.busy": "2026-01-27T19:50:15.504928Z",
     "iopub.status.idle": "2026-01-27T19:50:15.514938Z",
     "shell.execute_reply": "2026-01-27T19:50:15.513144Z",
     "shell.execute_reply.started": "2026-01-27T19:50:15.505455Z"
    }
   },
   "source": [
    "Which we can then again convert directly to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61afd2-af02-4be2-a8e9-76421edbea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list_of_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96193958-21eb-49e0-93f8-d8ebe020b146",
   "metadata": {},
   "source": [
    "Since the raw data already resemebles some data structure, we can use `read_json` directly. However, we specify `lines` as `True`, since as we discovered, the data consists of lines of `json` text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44898be-b2f3-47a3-87e0-115662bba049",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter = pd.read_json('data/sample_twitter.json.bz2', lines=True, nrows=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc34d001-89a4-43b6-aecd-30adf17f4cc9",
   "metadata": {},
   "source": [
    "### Writing `json` files using `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0b62c-5d6d-4b43-8f0e-883415e1a79c",
   "metadata": {},
   "source": [
    "Given any data frame, we can export it into a `json` format in several ways:\n",
    "\n",
    "- ‘split’ : dict like {{‘index’ -> [index], ‘columns’ -> [columns], ‘data’ -> [values]}}\n",
    "\n",
    "- ‘records’ : list like [{{column -> value}}, … , {{column -> value}}]\n",
    "\n",
    "- ‘index’ : dict like {{index -> {{column -> value}}}}\n",
    "\n",
    "- ‘columns’ : dict like {{column -> {{index -> value}}}}\n",
    "\n",
    "- ‘values’ : just the values array\n",
    "\n",
    "- ‘table’ : dict like {{‘schema’: {{schema}}, ‘data’: {{data}}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b35a3a-6414-4c6f-b7ed-6c16205af40f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b720d7ce-c824-4225-8c07-44cd7626e755",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_twitter.to_json(orient='split', indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b580d70-8a8d-4142-a139-c188f43a0c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_twitter.to_json(orient='records', indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00be496-08f9-4b67-bcce-cea6eb5949eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_twitter.to_json(orient='index', indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7829884-4b89-44af-93ea-f8bc5f74e109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_twitter.to_json(orient='columns', indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab5041-3bea-4cad-8d29-063671316983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_twitter.to_json(orient='values', indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daccaeb-2cff-44ea-853e-31486636260d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_twitter.to_json(orient='table', indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a5e88-a76a-41d6-a07b-dc7b020bd897",
   "metadata": {},
   "source": [
    "`to_json` outputs a string, as such to save it onto a text file, we need to write it to a file using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f19f7-ac2f-440b-b422-4fbd929e12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/sample_tweets.json', 'w') as f:\n",
    "    f.write(df_twitter.to_json(orient='records', indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c876b96-b9a5-4831-abf0-267e35e9cd38",
   "metadata": {},
   "source": [
    "That's it folks! We have shown how to load and wrangle data from CSV, Excel, and JSON files using Python and `pandas`, and how these steps enable basic data analysis. Mastering these workflows provides a strong foundation for working with real-world datasets and sets the stage for more advanced analytical techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d649f4-9b9e-43cd-a95b-c11e8457c3fa",
   "metadata": {},
   "source": [
    "<img src=\"images/banner-down.png\" style=\"width: 100%;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
